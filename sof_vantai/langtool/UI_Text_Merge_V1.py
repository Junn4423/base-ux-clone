"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    UI TEXT MERGE & OPTIMIZER v1.1.0 - ENHANCED
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Author: Junn4423
Version: 1.1.0 ENHANCED
License: Commercial
Date: 2025-10-03
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import os
import json
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, Set, List, Tuple
from datetime import datetime
import hashlib
import unicodedata

# ============================================
# VIETNAMESE DIACRITICS REMOVER
# ============================================

class VietnameseHelper:
    """Helper ƒë·ªÉ x·ª≠ l√Ω ti·∫øng Vi·ªát"""
    
    # B·∫£ng chuy·ªÉn ƒë·ªïi d·∫•u ti·∫øng Vi·ªát
    VIETNAMESE_MAP = {
        '√†': 'a', '√°': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'a', '·∫±': 'a', '·∫Ø': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
        '√¢': 'a', '·∫ß': 'a', '·∫•': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
        '√®': 'e', '√©': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': 'e', '·ªÅ': 'e', '·∫ø': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
        '√¨': 'i', '√≠': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≤': 'o', '√≥': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': 'o', '·ªì': 'o', '·ªë': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
        '∆°': 'o', '·ªù': 'o', '·ªõ': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
        '√π': 'u', '√∫': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': 'u', '·ª´': 'u', '·ª©': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
        '·ª≥': 'y', '√Ω': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
        'ƒë': 'd',
        '√Ä': 'A', '√Å': 'A', '·∫¢': 'A', '√É': 'A', '·∫†': 'A',
        'ƒÇ': 'A', '·∫∞': 'A', '·∫Æ': 'A', '·∫≤': 'A', '·∫¥': 'A', '·∫∂': 'A',
        '√Ç': 'A', '·∫¶': 'A', '·∫§': 'A', '·∫®': 'A', '·∫™': 'A', '·∫¨': 'A',
        '√à': 'E', '√â': 'E', '·∫∫': 'E', '·∫º': 'E', '·∫∏': 'E',
        '√ä': 'E', '·ªÄ': 'E', '·∫æ': 'E', '·ªÇ': 'E', '·ªÑ': 'E', '·ªÜ': 'E',
        '√å': 'I', '√ç': 'I', '·ªà': 'I', 'ƒ®': 'I', '·ªä': 'I',
        '√í': 'O', '√ì': 'O', '·ªé': 'O', '√ï': 'O', '·ªå': 'O',
        '√î': 'O', '·ªí': 'O', '·ªê': 'O', '·ªî': 'O', '·ªñ': 'O', '·ªò': 'O',
        '∆†': 'O', '·ªú': 'O', '·ªö': 'O', '·ªû': 'O', '·ª†': 'O', '·ª¢': 'O',
        '√ô': 'U', '√ö': 'U', '·ª¶': 'U', '≈®': 'U', '·ª§': 'U',
        '∆Ø': 'U', '·ª™': 'U', '·ª®': 'U', '·ª¨': 'U', '·ªÆ': 'U', '·ª∞': 'U',
        '·ª≤': 'Y', '√ù': 'Y', '·ª∂': 'Y', '·ª∏': 'Y', '·ª¥': 'Y',
        'ƒê': 'D',
    }
    
    @classmethod
    def remove_diacritics(cls, text: str) -> str:
        """
        B·ªè d·∫•u ti·∫øng Vi·ªát
        'b√†n nh√† h√†ng' -> 'ban nha hang'
        'caf√©' -> 'cafe'
        """
        result = []
        for char in text:
            if char in cls.VIETNAMESE_MAP:
                result.append(cls.VIETNAMESE_MAP[char])
            else:
                result.append(char)
        return ''.join(result)
    
    @classmethod
    def to_snake_case(cls, text: str) -> str:
        """
        Chuy·ªÉn text th√†nh snake_case kh√¥ng d·∫•u
        'B√†n nh√† h√†ng' -> 'ban_nha_hang'
        '123 ƒê∆∞·ªùng ABC' -> '123_duong_abc'
        """
        # B·ªè d·∫•u
        text = cls.remove_diacritics(text)
        
        # Chuy·ªÉn v·ªÅ lowercase
        text = text.lower()
        
        # Thay th·∫ø k√Ω t·ª± ƒë·∫∑c bi·ªát v√† kho·∫£ng tr·∫Øng b·∫±ng _
        text = re.sub(r'[^\w\s]', '_', text)
        text = re.sub(r'\s+', '_', text)
        text = re.sub(r'_+', '_', text)  # Remove multiple underscores
        
        # X√≥a _ ·ªü ƒë·∫ßu v√† cu·ªëi
        text = text.strip('_')
        
        return text

# ============================================
# TEXT CATEGORIZER (AI-based)
# ============================================

class TextCategorizer:
    """Ph√¢n lo·∫°i text th√†nh c√°c category h·ª£p l√Ω"""
    
    # ƒê·ªãnh nghƒ©a categories v√† keywords
    CATEGORIES = {
        'common': {
            'name': 'Common',
            'keywords': ['ok', 'yes', 'no', 'cancel', 'close', 'back', 'next', 'prev', 'previous', 'continue', 
                        'ƒë·ªìng √Ω', 'kh√¥ng', 'h·ªßy', 'ƒë√≥ng', 'quay l·∫°i', 'ti·∫øp', 'ti·∫øp t·ª•c', 'tr∆∞·ªõc', 'sau'],
            'subcategories': ['button', 'action', 'navigation']
        },
        'auth': {
            'name': 'Authentication',
            'keywords': ['login', 'logout', 'signin', 'signout', 'signup', 'register', 'password', 'username',
                        'ƒëƒÉng nh·∫≠p', 'ƒëƒÉng xu·∫•t', 'ƒëƒÉng k√Ω', 't√†i kho·∫£n', 'm·∫≠t kh·∫©u', 'ng∆∞·ªùi d√πng'],
            'subcategories': ['login', 'register', 'forgot_password', 'profile']
        },
        'form': {
            'name': 'Form',
            'keywords': ['name', 'email', 'phone', 'address', 'date', 'time', 'select', 'choose', 'input',
                        't√™n', 'h·ªç', 'ƒë·ªãa ch·ªâ', 'ƒëi·ªán tho·∫°i', 'email', 'ng√†y', 'gi·ªù', 'ch·ªçn', 'nh·∫≠p'],
            'subcategories': ['label', 'placeholder', 'validation', 'helper']
        },
        'message': {
            'name': 'Messages',
            'keywords': ['success', 'error', 'warning', 'info', 'alert', 'notification',
                        'th√†nh c√¥ng', 'l·ªói', 'c·∫£nh b√°o', 'th√¥ng b√°o', 'ch√∫ √Ω'],
            'subcategories': ['success', 'error', 'warning', 'info']
        },
        'crud': {
            'name': 'CRUD Operations',
            'keywords': ['create', 'add', 'new', 'edit', 'update', 'delete', 'remove', 'save', 'submit',
                        'th√™m', 't·∫°o', 'm·ªõi', 's·ª≠a', 'c·∫≠p nh·∫≠t', 'x√≥a', 'l∆∞u', 'g·ª≠i'],
            'subcategories': ['create', 'read', 'update', 'delete']
        },
        'list': {
            'name': 'List & Table',
            'keywords': ['list', 'table', 'grid', 'row', 'column', 'sort', 'filter', 'search', 'page', 'total',
                        'danh s√°ch', 'b·∫£ng', 'h√†ng', 'c·ªôt', 's·∫Øp x·∫øp', 'l·ªçc', 't√¨m ki·∫øm', 'trang', 't·ªïng'],
            'subcategories': ['header', 'action', 'filter', 'pagination']
        },
        'menu': {
            'name': 'Menu & Navigation',
            'keywords': ['home', 'dashboard', 'menu', 'settings', 'help', 'about', 'contact',
                        'trang ch·ªß', 'b·∫£ng ƒëi·ªÅu khi·ªÉn', 'menu', 'c√†i ƒë·∫∑t', 'tr·ª£ gi√∫p', 'li√™n h·ªá'],
            'subcategories': ['main', 'user', 'admin']
        },
        'status': {
            'name': 'Status',
            'keywords': ['active', 'inactive', 'pending', 'processing', 'completed', 'failed', 'cancelled',
                        'ƒëang ho·∫°t ƒë·ªông', 'ng·ª´ng', 'ch·ªù', 'ƒëang x·ª≠ l√Ω', 'ho√†n th√†nh', 'th·∫•t b·∫°i', 'h·ªßy'],
            'subcategories': ['state', 'progress']
        },
        'time': {
            'name': 'Date & Time',
            'keywords': ['today', 'yesterday', 'tomorrow', 'now', 'date', 'time', 'hour', 'minute',
                        'h√¥m nay', 'h√¥m qua', 'ng√†y mai', 'b√¢y gi·ªù', 'ng√†y', 'gi·ªù', 'ph√∫t', 'gi√¢y'],
            'subcategories': ['relative', 'absolute', 'format']
        },
        'business': {
            'name': 'Business',
            'keywords': ['product', 'service', 'price', 'payment', 'order', 'invoice', 'customer',
                        's·∫£n ph·∫©m', 'd·ªãch v·ª•', 'gi√°', 'thanh to√°n', 'ƒë∆°n h√†ng', 'h√≥a ƒë∆°n', 'kh√°ch h√†ng'],
            'subcategories': ['product', 'order', 'customer', 'payment']
        },
        'report': {
            'name': 'Reports',
            'keywords': ['report', 'export', 'print', 'download', 'pdf', 'excel', 'statistics',
                        'b√°o c√°o', 'xu·∫•t', 'in', 't·∫£i xu·ªëng', 'th·ªëng k√™'],
            'subcategories': ['export', 'print', 'statistics']
        },
    }
    
    @classmethod
    def categorize(cls, text: str) -> Tuple[str, str]:
        """
        Ph√¢n lo·∫°i text v√†o category ph√π h·ª£p
        Returns: (category, subcategory)
        """
        text_lower = text.lower()
        
        # Score m·ªói category
        scores = defaultdict(lambda: {'score': 0, 'subcategory': 'general'})
        
        for category, config in cls.CATEGORIES.items():
            for keyword in config['keywords']:
                if keyword in text_lower:
                    scores[category]['score'] += 1
        
        # T√¨m category c√≥ score cao nh·∫•t
        if scores:
            best_category = max(scores.items(), key=lambda x: x[1]['score'])
            if best_category[1]['score'] > 0:
                category = best_category[0]
                # Ch·ªçn subcategory ph√π h·ª£p
                subcategory = cls._detect_subcategory(text_lower, category)
                return category, subcategory
        
        # Default
        return 'other', 'general'
    
    @classmethod
    def _detect_subcategory(cls, text_lower: str, category: str) -> str:
        """Ph√°t hi·ªán subcategory chi ti·∫øt h∆°n"""
        if category == 'common':
            if any(word in text_lower for word in ['ok', 'yes', 'no', 'ƒë·ªìng √Ω', 'kh√¥ng']):
                return 'button'
            elif any(word in text_lower for word in ['next', 'back', 'prev', 'ti·∫øp', 'quay l·∫°i']):
                return 'navigation'
            else:
                return 'action'
        
        elif category == 'message':
            if any(word in text_lower for word in ['success', 'th√†nh c√¥ng', 'ho√†n th√†nh']):
                return 'success'
            elif any(word in text_lower for word in ['error', 'l·ªói', 'th·∫•t b·∫°i']):
                return 'error'
            elif any(word in text_lower for word in ['warning', 'c·∫£nh b√°o']):
                return 'warning'
            else:
                return 'info'
        
        elif category == 'crud':
            if any(word in text_lower for word in ['add', 'create', 'new', 'th√™m', 't·∫°o']):
                return 'create'
            elif any(word in text_lower for word in ['edit', 'update', 's·ª≠a', 'c·∫≠p nh·∫≠t']):
                return 'update'
            elif any(word in text_lower for word in ['delete', 'remove', 'x√≥a']):
                return 'delete'
            else:
                return 'read'
        
        elif category == 'form':
            if any(word in text_lower for word in ['placeholder', 'nh·∫≠p', 'enter']):
                return 'placeholder'
            elif any(word in text_lower for word in ['required', 'invalid', 'b·∫Øt bu·ªôc', 'kh√¥ng h·ª£p l·ªá']):
                return 'validation'
            else:
                return 'label'
        
        elif category == 'list':
            if any(word in text_lower for word in ['filter', 'search', 'l·ªçc', 't√¨m']):
                return 'filter'
            elif any(word in text_lower for word in ['page', 'total', 'trang', 't·ªïng']):
                return 'pagination'
            elif any(word in text_lower for word in ['edit', 'delete', 'view', 's·ª≠a', 'x√≥a', 'xem']):
                return 'action'
            else:
                return 'header'
        
        # Default subcategory
        return 'general'

# ============================================
# TEXT MERGER
# ============================================

class TextMerger:
    """Merge text tr√πng l·∫∑p v√† t·∫°o key mapping"""
    
    def __init__(self):
        self.merged_texts = {}  # {lang: {category: {subcategory: {key: text}}}}
        self.duplicates_removed = 0
        self.total_texts = 0
        self.key_counter = defaultdict(int)
    
    def merge(self, results: Dict[str, Dict[str, Set[str]]]) -> Dict[str, Dict]:
        """
        Merge all texts and organize by category
        results: {module_name: {lang: set_of_texts}}
        returns: {lang: {category: {subcategory: {key: text}}}}
        """
        print("\n" + "="*80)
        print("MERGING & ORGANIZING TEXTS")
        print("="*80 + "\n")
        
        # Collect all unique texts per language
        lang_texts = defaultdict(set)
        for module_name, lang_dict in results.items():
            for lang, texts in lang_dict.items():
                lang_texts[lang].update(texts)
                self.total_texts += len(texts)
        
        # Process each language
        for lang, texts in lang_texts.items():
            print(f"Processing {lang}: {len(texts)} texts")
            self.merged_texts[lang] = defaultdict(lambda: defaultdict(dict))
            
            for text in sorted(texts):
                # Categorize
                category, subcategory = TextCategorizer.categorize(text)
                
                # Generate key
                key = self._generate_key(text, category, subcategory)
                
                # Store
                self.merged_texts[lang][category][subcategory][key] = text
        
        # Calculate duplicates removed
        total_merged = sum(
            len(subcat_dict)
            for lang_dict in self.merged_texts.values()
            for cat_dict in lang_dict.values()
            for subcat_dict in cat_dict.values()
        )
        self.duplicates_removed = self.total_texts - total_merged
        
        print(f"\nOriginal texts: {self.total_texts}")
        print(f"After merge: {total_merged}")
        print(f"Duplicates removed: {self.duplicates_removed}")
        
        return self.merged_texts
    
    def _generate_key(self, text: str, category: str, subcategory: str) -> str:
        """
        Generate unique key for text WITHOUT diacritics
        X·ª≠ l√Ω tr√πng l·∫∑p khi b·ªè d·∫•u (b√°n/b√†n -> ban_1, ban_2)
        """
        # B·ªè d·∫•u ti·∫øng Vi·ªát v√† chuy·ªÉn v·ªÅ snake_case
        clean_text = VietnameseHelper.to_snake_case(text)
        
        # L·∫•y t·ªëi ƒëa 3 t·ª´ ƒë·∫ßu ti√™n
        words = clean_text.split('_')[:3]
        
        if not words or not words[0]:
            words = ['text']
        
        # Create base key (kh√¥ng d·∫•u)
        base_key = '_'.join(words)
        
        # T·∫°o full key ƒë·ªÉ check duplicate trong C√ôNG category + subcategory
        scope_key = f"{category}.{subcategory}.{base_key}"
        
        # Ki·ªÉm tra tr√πng l·∫∑p
        if scope_key in self.key_counter:
            # C√≥ tr√πng r·ªìi -> th√™m suffix s·ªë
            self.key_counter[scope_key] += 1
            final_key = f"{base_key}_{self.key_counter[scope_key]}"
        else:
            # L·∫ßn ƒë·∫ßu ti√™n
            self.key_counter[scope_key] = 0
            final_key = base_key
        
        return final_key

# ============================================
# EXPORTER
# ============================================

class MergeExporter:
    """Export merged texts to various formats"""
    
    @staticmethod
    def export_json_nested(merged_texts: Dict, output_dir: Path):
        """Export as nested JSON (for easy import)"""
        print("\n" + "="*80)
        print("EXPORTING MERGED TEXTS")
        print("="*80 + "\n")
        
        output_dir = output_dir / "merged"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Export per language
        for lang, categories in merged_texts.items():
            filepath = output_dir / f"{lang}.json"
            
            # Add metadata
            output = {
                'metadata': {
                    'language': lang,
                    'version': '1.1.0',
                    'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'total_texts': sum(
                        len(subcat_dict)
                        for cat_dict in categories.values()
                        for subcat_dict in cat_dict.values()
                    ),
                    'features': [
                        'No Vietnamese diacritics in keys',
                        'Smart duplicate handling with suffix',
                        'Snake_case normalization',
                        'Code-friendly key format',
                    ],
                    'note': 'All keys are without diacritics. Example: "b√†n nh√† h√†ng" -> "ban_nha_hang"',
                },
                'texts': dict(categories)
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
            
            print(f" {filepath.name} - {output['metadata']['total_texts']} texts")
    
    @staticmethod
    def export_flat_json(merged_texts: Dict, output_dir: Path):
        """Export as flat JSON (key: text)"""
        output_dir = output_dir / "merged"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for lang, categories in merged_texts.items():
            filepath = output_dir / f"{lang}_flat.json"
            
            # Flatten structure
            flat = {}
            for category, subcategories in categories.items():
                for subcategory, texts in subcategories.items():
                    for key, text in texts.items():
                        full_key = f"{category}.{subcategory}.{key}"
                        flat[full_key] = text
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(flat, f, ensure_ascii=False, indent=2)
            
            print(f" {filepath.name} - {len(flat)} texts (flat)")
    
    @staticmethod
    def export_typescript_constants(merged_texts: Dict, output_dir: Path):
        """Export as TypeScript constants"""
        output_dir = output_dir / "merged"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for lang, categories in merged_texts.items():
            filepath = output_dir / f"{lang}.ts"
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("// Auto-generated language constants\n")
                f.write(f"// Language: {lang}\n")
                f.write(f"// Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("// NOTE: All keys are without Vietnamese diacritics\n")
                f.write("// Example: 'b√†n nh√† h√†ng' -> ban_nha_hang\n\n")
                
                f.write(f"export const {lang.upper()} = {{\n")
                
                for category, subcategories in sorted(categories.items()):
                    f.write(f"  {category}: {{\n")
                    
                    for subcategory, texts in sorted(subcategories.items()):
                        f.write(f"    {subcategory}: {{\n")
                        
                        for key, text in sorted(texts.items()):
                            # Escape quotes
                            escaped = text.replace("'", "\\'").replace('"', '\\"')
                            f.write(f"      {key}: '{escaped}',\n")
                        
                        f.write(f"    }},\n")
                    
                    f.write(f"  }},\n")
                
                f.write("};\n")
            
            print(f" {filepath.name} - TypeScript constants")
    
    @staticmethod
    def export_summary(merged_texts: Dict, output_dir: Path, stats: Dict):
        """Export summary report"""
        output_dir = output_dir / "merged"
        filepath = output_dir / "SUMMARY.txt"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("TEXT MERGE SUMMARY v1.1.0\n")
            f.write("="*80 + "\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            f.write("STATISTICS\n")
            f.write("-"*80 + "\n")
            f.write(f"Original texts:      {stats['total_texts']}\n")
            f.write(f"After merge:         {stats['merged_texts']}\n")
            f.write(f"Duplicates removed:  {stats['duplicates_removed']}\n")
            f.write(f"Reduction rate:      {stats['reduction_rate']:.1f}%\n")
            f.write("\n")
            
            f.write("BY LANGUAGE\n")
            f.write("-"*80 + "\n")
            for lang, categories in sorted(merged_texts.items()):
                total = sum(
                    len(texts)
                    for subcategories in categories.values()
                    for texts in subcategories.values()
                )
                f.write(f"{lang:10} {total:6} texts\n")
            f.write("\n")
            
            f.write("BY CATEGORY\n")
            f.write("-"*80 + "\n")
            
            for lang, categories in sorted(merged_texts.items()):
                f.write(f"\n{lang.upper()}:\n")
                for category, subcategories in sorted(categories.items()):
                    total = sum(len(texts) for texts in subcategories.values())
                    f.write(f"  {category:20} {total:6} texts\n")
                    
                    for subcategory, texts in sorted(subcategories.items()):
                        f.write(f"    ‚îî‚îÄ {subcategory:15} {len(texts):6} texts\n")
        
        print(f" {filepath.name} - Summary report")

# ============================================
# MAIN MERGE FUNCTION
# ============================================

def run_merge(output_dir: Path, results: Dict[str, Dict[str, Set[str]]]):
    """Main merge function called from extractor"""
    print("\n" + "="*80)
    print("UI TEXT MERGE & OPTIMIZER v1.1.0 - ENHANCED")
    print("="*80 + "\n")
    
    # Step 1: Merge texts
    merger = TextMerger()
    merged_texts = merger.merge(results)
    
    # Step 2: Export to different formats
    MergeExporter.export_json_nested(merged_texts, output_dir)
    MergeExporter.export_flat_json(merged_texts, output_dir)
    MergeExporter.export_typescript_constants(merged_texts, output_dir)
    
    # Step 3: Export summary
    total_merged = sum(
        len(texts)
        for lang_dict in merged_texts.values()
        for cat_dict in lang_dict.values()
        for texts in cat_dict.values()
    )
    
    stats = {
        'total_texts': merger.total_texts,
        'merged_texts': total_merged,
        'duplicates_removed': merger.duplicates_removed,
        'reduction_rate': (merger.duplicates_removed / merger.total_texts * 100) if merger.total_texts > 0 else 0,
    }
    
    MergeExporter.export_summary(merged_texts, output_dir, stats)
    
    # Final report
    print("\n" + "="*80)
    print("MERGE COMPLETE!")
    print("="*80)
    print(f"Original texts:     {stats['total_texts']}")
    print(f"After merge:        {stats['merged_texts']}")
    print(f"Duplicates removed: {stats['duplicates_removed']}")
    print(f"Reduction rate:     {stats['reduction_rate']:.1f}%")
    print(f"Output location:    {output_dir / 'merged'}")
    print("="*80 + "\n")

    print("Exported formats:")
    print("JSON nested (easy to navigate)")
    print("JSON flat (key: text)")
    print("TypeScript constants (.ts)")
    print("Summary report (SUMMARY.txt)")
    print("\n" + "="*80 + "\n")

# ============================================
# STANDALONE USAGE
# ============================================

if __name__ == "__main__":
    print("‚ö†Ô∏è  This tool should be called from UI_Text_Extractor_V4")
    print("üí° Or you can manually import and use run_merge() function")
